{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28972472-453e-42fb-8adf-a69a0a41b7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GPU INITIALIZATION\n",
      "======================================================================\n",
      "GPU Device: NVIDIA GeForce RTX 3060\n",
      "GPU Memory: 12.88 GB\n",
      "CUDA Version: 12.1\n",
      "CUDA enabled - All processing will run on GPU!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "CUDA-ACCELERATED LUNG DISEASE CLASSIFICATION\n",
      "PyTorch CUDA - All Processing on GPU - LOGISTIC REGRESSION\n",
      "======================================================================\n",
      "\n",
      "Initial GPU Memory: 0.00 GB allocated\n",
      "Initial GPU Memory: 0.00 GB reserved\n",
      "======================================================================\n",
      "DATASET LOADING AND QUALITY CHECK\n",
      "======================================================================\n",
      "\n",
      "Processing Normal...\n",
      "\n",
      "Processing Pneumonia_bacterial...\n",
      "\n",
      "Processing Pneumonia_viral...\n",
      "\n",
      "======================================================================\n",
      "DATASET STATISTICS\n",
      "======================================================================\n",
      "Normal:\n",
      "  Total: 1583\n",
      "  Loaded: 1583\n",
      "  Rejected: 0\n",
      "Pneumonia_bacterial:\n",
      "  Total: 2780\n",
      "  Loaded: 2780\n",
      "  Rejected: 0\n",
      "Pneumonia_viral:\n",
      "  Total: 1493\n",
      "  Loaded: 1493\n",
      "  Rejected: 0\n",
      "\n",
      "Transferring data to GPU...\n",
      "Final Dataset Shape: torch.Size([5856, 256, 256])\n",
      "Labels Shape: torch.Size([5856])\n",
      "Data location: cuda:0\n",
      "GPU Memory after loading: 1.54 GB allocated\n",
      "\n",
      "======================================================================\n",
      "DATA AUGMENTATION (GPU-Accelerated)\n",
      "======================================================================\n",
      "Applying CLAHE enhancement...\n",
      " Processing 0/5856...\n",
      " Processing 500/5856...\n",
      " Processing 1000/5856...\n",
      " Processing 1500/5856...\n",
      " Processing 2000/5856...\n",
      " Processing 2500/5856...\n",
      " Processing 3000/5856...\n",
      " Processing 3500/5856...\n",
      " Processing 4000/5856...\n",
      " Processing 4500/5856...\n",
      " Processing 5000/5856...\n",
      " Processing 5500/5856...\n",
      "Applying rotation augmentation...\n",
      " Processing 0/5856...\n",
      " Processing 500/5856...\n",
      " Processing 1000/5856...\n",
      " Processing 1500/5856...\n",
      " Processing 2000/5856...\n",
      " Processing 2500/5856...\n",
      " Processing 3000/5856...\n",
      " Processing 3500/5856...\n",
      " Processing 4000/5856...\n",
      " Processing 4500/5856...\n",
      " Processing 5000/5856...\n",
      " Processing 5500/5856...\n",
      "\n",
      "Original dataset size: 5856\n",
      "Augmented dataset size: 17568\n",
      "Augmentation multiplier: 3.0x\n",
      "Data stored on: cuda:0\n",
      "GPU Memory after augmentation: 6.14 GB allocated\n",
      "\n",
      "======================================================================\n",
      "OUTLIER REMOVAL (IQR Method - GPU)\n",
      "======================================================================\n",
      "Original samples: 17568\n",
      "Outliers removed: 317\n",
      "Remaining samples: 17251\n",
      "\n",
      "======================================================================\n",
      "IMAGE NORMALIZATION (Z-Score - GPU)\n",
      "======================================================================\n",
      "Global Mean: 123.19\n",
      "Global Std: 63.41\n",
      "GPU Memory after preprocessing: 6.06 GB allocated\n",
      "\n",
      "======================================================================\n",
      "RADIOMIC FEATURE EXTRACTION (GPU-Accelerated)\n",
      "======================================================================\n",
      "Feature Categories:\n",
      " 1. Statistical: Mean, Variance, Std\n",
      " 2. Texture: GLCM Contrast, Homogeneity\n",
      " 3. Filter-based: Gabor Mean Response, Gabor Std Response\n",
      "======================================================================\n",
      "Extracting features from image 0/17251...\n",
      "Extracting features from image 500/17251...\n",
      "Extracting features from image 1000/17251...\n",
      "Extracting features from image 1500/17251...\n",
      "Extracting features from image 2000/17251...\n",
      "Extracting features from image 2500/17251...\n",
      "Extracting features from image 3000/17251...\n",
      "Extracting features from image 3500/17251...\n",
      "Extracting features from image 4000/17251...\n",
      "Extracting features from image 4500/17251...\n",
      "Extracting features from image 5000/17251...\n",
      "Extracting features from image 5500/17251...\n",
      "Extracting features from image 6000/17251...\n",
      "Extracting features from image 6500/17251...\n",
      "Extracting features from image 7000/17251...\n",
      "Extracting features from image 7500/17251...\n",
      "Extracting features from image 8000/17251...\n",
      "Extracting features from image 8500/17251...\n",
      "Extracting features from image 9000/17251...\n",
      "Extracting features from image 9500/17251...\n",
      "Extracting features from image 10000/17251...\n",
      "Extracting features from image 10500/17251...\n",
      "Extracting features from image 11000/17251...\n",
      "Extracting features from image 11500/17251...\n",
      "Extracting features from image 12000/17251...\n",
      "Extracting features from image 12500/17251...\n",
      "Extracting features from image 13000/17251...\n",
      "Extracting features from image 13500/17251...\n",
      "Extracting features from image 14000/17251...\n",
      "Extracting features from image 14500/17251...\n",
      "Extracting features from image 15000/17251...\n",
      "Extracting features from image 15500/17251...\n",
      "Extracting features from image 16000/17251...\n",
      "Extracting features from image 16500/17251...\n",
      "Extracting features from image 17000/17251...\n",
      "\n",
      "Total features extracted per image: 7\n",
      "All features stored on: cuda:0\n",
      "GPU Memory after feature extraction: 1.54 GB allocated\n",
      "\n",
      "======================================================================\n",
      "FEATURE SCALING (GPU - Z-Score)\n",
      "======================================================================\n",
      "Fitted scaling parameters on GPU\n",
      "Scaled features shape: torch.Size([17251, 7])\n",
      "Mean: 0.0000\n",
      "Std: 1.0000\n",
      "\n",
      "======================================================================\n",
      "DIMENSIONALITY REDUCTION (PCA on GPU - 7 components)\n",
      "======================================================================\n",
      "Original dimension: 7\n",
      "Reduced dimension: 7\n",
      "Total variance explained: 1.0000\n",
      "Computation done on: GPU\n",
      "GPU Memory after PCA: 1.54 GB allocated\n",
      "\n",
      "======================================================================\n",
      "TRAINING WITH 70% TRAIN / 30% TEST SPLIT\n",
      "======================================================================\n",
      "\n",
      "Train set: 12075 samples (GPU)\n",
      "Test set: 5176 samples (GPU)\n",
      "GPU Memory before training: 1.54 GB allocated\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "TRAINING: Logistic Regression on GPU (Multiclass)\n",
      "----------------------------------------------------------------------\n",
      "  Iteration 0: Loss = 1.1002, Accuracy = 0.2676\n",
      "  Iteration 500: Loss = 0.9832, Accuracy = 0.5187\n",
      "  Iteration 1000: Loss = 0.9746, Accuracy = 0.5255\n",
      "  Iteration 1500: Loss = 0.9700, Accuracy = 0.5308\n",
      "Logistic Regression training completed on GPU!\n",
      "\n",
      "GPU Memory after training: 1.54 GB allocated\n",
      "\n",
      "======================================================================\n",
      "RESULTS (CUDA-Accelerated with PyTorch)\n",
      "======================================================================\n",
      "Accuracy: 0.5396\n",
      "Training Time: 1.02s\n",
      "Processing: 100% on NVIDIA GeForce RTX 3060\n",
      "\n",
      "Normal:\n",
      "  Precision: 0.5347\n",
      "  Recall: 0.4133\n",
      "  F1-Score: 0.4663\n",
      "\n",
      "Bacterial Pneumonia:\n",
      "  Precision: 0.5433\n",
      "  Recall: 0.8120\n",
      "  F1-Score: 0.6510\n",
      "\n",
      "Viral Pneumonia:\n",
      "  Precision: 0.5210\n",
      "  Recall: 0.1719\n",
      "  F1-Score: 0.2586\n",
      "\n",
      "Results saved to: output_lr_cuda\\results_summary_lr_cuda.json\n",
      "Visualizations saved to: output_lr_cuda/\n",
      "\n",
      "Final GPU Memory: 1.55 GB allocated\n",
      "Peak GPU Memory: 15.10 GB\n",
      "\n",
      "======================================================================\n",
      "CUDA-ACCELERATED PIPELINE COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch CUDA support\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        GPU_AVAILABLE = True\n",
    "        device = torch.device('cuda:0')\n",
    "        print(\"=\" * 70)\n",
    "        print(\"GPU INITIALIZATION\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "        print(\"CUDA enabled - All processing will run on GPU!\")\n",
    "        print(\"=\" * 70)\n",
    "    else:\n",
    "        GPU_AVAILABLE = False\n",
    "        device = torch.device('cpu')\n",
    "        print(\"WARNING: CUDA not available. Using CPU.\")\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    device = torch.device('cpu')\n",
    "    print(\"ERROR: PyTorch not available. Please install with: pip install torch torchvision\")\n",
    "    exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. DATASET PREPARATION AND LOADING\n",
    "# ============================================================================\n",
    "class DatasetLoader:\n",
    "    \"\"\"Handles dataset loading with GPU transfer\"\"\"\n",
    "  \n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.categories = ['Normal', 'Pneumonia_bacterial', 'Pneumonia_viral']\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.label_map = {'Normal': 0, 'Pneumonia_bacterial': 1, 'Pneumonia_viral': 2}\n",
    "      \n",
    "    def check_image_quality(self, img_path):\n",
    "        try:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                return False, \"Corrupted\"\n",
    "            if img.shape[0] < 64 or img.shape[1] < 64:\n",
    "                return False, \"Low resolution\"\n",
    "            mean_intensity = np.mean(img)\n",
    "            if mean_intensity < 10 or mean_intensity > 245:\n",
    "                return False, \"Poor contrast\"\n",
    "            return True, \"OK\"\n",
    "        except Exception as e:\n",
    "            return False, str(e)\n",
    "  \n",
    "    def load_dataset(self, target_size=(256, 256), max_samples_per_class=None):\n",
    "        print(\"=\" * 70)\n",
    "        print(\"DATASET LOADING AND QUALITY CHECK\")\n",
    "        print(\"=\" * 70)\n",
    "      \n",
    "        stats = {cat: {'total': 0, 'loaded': 0, 'rejected': 0} for cat in self.categories}\n",
    "      \n",
    "        for category in self.categories:\n",
    "            cat_path = os.path.join(self.dataset_path, category)\n",
    "            if not os.path.exists(cat_path):\n",
    "                print(f\"Warning: {category} folder not found!\")\n",
    "                continue\n",
    "          \n",
    "            files = [f for f in os.listdir(cat_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            stats[category]['total'] = len(files)\n",
    "           \n",
    "            if max_samples_per_class is not None:\n",
    "                files = files[:max_samples_per_class]\n",
    "          \n",
    "            print(f\"\\nProcessing {category}...\")\n",
    "            for filename in files:\n",
    "                img_path = os.path.join(cat_path, filename)\n",
    "                is_valid, reason = self.check_image_quality(img_path)\n",
    "              \n",
    "                if is_valid:\n",
    "                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    img_resized = cv2.resize(img, target_size)\n",
    "                    self.images.append(img_resized)\n",
    "                    self.labels.append(self.label_map[category])\n",
    "                    stats[category]['loaded'] += 1\n",
    "                else:\n",
    "                    stats[category]['rejected'] += 1\n",
    "      \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"DATASET STATISTICS\")\n",
    "        print(\"=\" * 70)\n",
    "        for cat in self.categories:\n",
    "            print(f\"{cat}:\")\n",
    "            print(f\"  Total: {stats[cat]['total']}\")\n",
    "            print(f\"  Loaded: {stats[cat]['loaded']}\")\n",
    "            print(f\"  Rejected: {stats[cat]['rejected']}\")\n",
    "      \n",
    "        images_np = np.array(self.images, dtype=np.float32)\n",
    "        labels_np = np.array(self.labels, dtype=np.int64)\n",
    "      \n",
    "        print(f\"\\nTransferring data to GPU...\")\n",
    "        self.images = torch.from_numpy(images_np).to(device)\n",
    "        self.labels = torch.from_numpy(labels_np).to(device)\n",
    "      \n",
    "        print(f\"Final Dataset Shape: {self.images.shape}\")\n",
    "        print(f\"Labels Shape: {self.labels.shape}\")\n",
    "        print(f\"Data location: {self.images.device}\")\n",
    "      \n",
    "        return self.images, self.labels\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DATA PREPROCESSING (GPU-Optimized)\n",
    "# ============================================================================\n",
    "class DataPreprocessor:\n",
    "  \n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "  \n",
    "    def apply_clahe_batch(self, images):\n",
    "        results = []\n",
    "        for i in range(len(images)):\n",
    "            img_cpu = images[i].cpu().numpy().astype(np.uint8)\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "            result = clahe.apply(img_cpu)\n",
    "            results.append(result)\n",
    "        return torch.from_numpy(np.stack(results)).to(device)\n",
    "  \n",
    "    def add_gaussian_noise_gpu(self, images, mean=0, sigma=10):\n",
    "        noise = torch.randn_like(images) * sigma + mean\n",
    "        noisy_img = images + noise\n",
    "        return torch.clamp(noisy_img, 0, 255)\n",
    "  \n",
    "    def rotate_image_batch(self, images, angle):\n",
    "        results = []\n",
    "        for i in range(len(images)):\n",
    "            img_cpu = images[i].cpu().numpy().astype(np.uint8)\n",
    "            h, w = img_cpu.shape\n",
    "            center = (w // 2, h // 2)\n",
    "            M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "            rotated = cv2.warpAffine(img_cpu, M, (w, h))\n",
    "            results.append(rotated)\n",
    "        return torch.from_numpy(np.stack(results)).to(device)\n",
    "  \n",
    "    def flip_image_gpu(self, images, direction='horizontal'):\n",
    "        if direction == 'horizontal':\n",
    "            return torch.flip(images, dims=[2])\n",
    "        else:\n",
    "            return torch.flip(images, dims=[1])\n",
    "  \n",
    "    def augment_data(self, images, labels, augmentation_factor=0):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"DATA AUGMENTATION (GPU-Accelerated)\")\n",
    "        print(\"=\" * 70)\n",
    "      \n",
    "        n_original = len(images)\n",
    "       \n",
    "        if augmentation_factor == 0:\n",
    "            print(\"No augmentation applied - using original data only\")\n",
    "            return images, labels\n",
    "       \n",
    "        batch_size = 50\n",
    "      \n",
    "        final_images = images\n",
    "        final_labels = labels\n",
    "       \n",
    "        if augmentation_factor >= 1:\n",
    "            print(\"Applying CLAHE enhancement...\")\n",
    "            clahe_results = []\n",
    "            for i in range(0, n_original, batch_size):\n",
    "                if i % 500 == 0:\n",
    "                    print(f\" Processing {i}/{n_original}...\")\n",
    "                batch = images[i:i+batch_size]\n",
    "                clahe_batch = self.apply_clahe_batch(batch)\n",
    "                clahe_results.append(clahe_batch)\n",
    "                if GPU_AVAILABLE:\n",
    "                    torch.cuda.empty_cache()\n",
    "           \n",
    "            clahe_images = torch.cat(clahe_results, dim=0)\n",
    "            del clahe_results\n",
    "            if GPU_AVAILABLE:\n",
    "                torch.cuda.empty_cache()\n",
    "           \n",
    "            final_images = torch.cat([final_images, clahe_images], dim=0)\n",
    "            final_labels = torch.cat([final_labels, labels], dim=0)\n",
    "            del clahe_images\n",
    "            if GPU_AVAILABLE:\n",
    "                torch.cuda.empty_cache()\n",
    "      \n",
    "        if augmentation_factor >= 2:\n",
    "            print(\"Applying rotation augmentation...\")\n",
    "            rotation_results = []\n",
    "            for i in range(0, n_original, batch_size):\n",
    "                if i % 500 == 0:\n",
    "                    print(f\" Processing {i}/{n_original}...\")\n",
    "                batch = images[i:i+batch_size]\n",
    "                rot_batch = self.rotate_image_batch(batch, 15)\n",
    "                rotation_results.append(rot_batch)\n",
    "                if GPU_AVAILABLE:\n",
    "                    torch.cuda.empty_cache()\n",
    "           \n",
    "            rotation_images = torch.cat(rotation_results, dim=0)\n",
    "            del rotation_results\n",
    "            if GPU_AVAILABLE:\n",
    "                torch.cuda.empty_cache()\n",
    "               \n",
    "            final_images = torch.cat([final_images, rotation_images], dim=0)\n",
    "            final_labels = torch.cat([final_labels, labels], dim=0)\n",
    "            del rotation_images\n",
    "            if GPU_AVAILABLE:\n",
    "                torch.cuda.empty_cache()\n",
    "      \n",
    "        if augmentation_factor >= 3:\n",
    "            print(\"Applying flip augmentation...\")\n",
    "            flip_results = []\n",
    "            for i in range(0, n_original, batch_size):\n",
    "                if i % 500 == 0:\n",
    "                    print(f\" Processing {i}/{n_original}...\")\n",
    "                batch = images[i:i+batch_size]\n",
    "                batch_3d = batch.unsqueeze(1)\n",
    "                flipped = torch.flip(batch_3d, dims=[3]).squeeze(1)\n",
    "                flip_results.append(flipped)\n",
    "                if GPU_AVAILABLE:\n",
    "                    torch.cuda.empty_cache()\n",
    "           \n",
    "            flip_images = torch.cat(flip_results, dim=0)\n",
    "            del flip_results\n",
    "            if GPU_AVAILABLE:\n",
    "                torch.cuda.empty_cache()\n",
    "               \n",
    "            final_images = torch.cat([final_images, flip_images], dim=0)\n",
    "            final_labels = torch.cat([final_labels, labels], dim=0)\n",
    "            del flip_images\n",
    "            if GPU_AVAILABLE:\n",
    "                torch.cuda.empty_cache()\n",
    "      \n",
    "        if augmentation_factor >= 4:\n",
    "            print(\"Applying noise augmentation...\")\n",
    "            noise_results = []\n",
    "            for i in range(0, n_original, batch_size):\n",
    "                if i % 500 == 0:\n",
    "                    print(f\" Processing {i}/{n_original}...\")\n",
    "                batch = images[i:i+batch_size]\n",
    "                noisy = self.add_gaussian_noise_gpu(batch, sigma=5)\n",
    "                noise_results.append(noisy)\n",
    "                if GPU_AVAILABLE:\n",
    "                    torch.cuda.empty_cache()\n",
    "           \n",
    "            noise_images = torch.cat(noise_results, dim=0)\n",
    "            del noise_results\n",
    "            if GPU_AVAILABLE:\n",
    "                torch.cuda.empty_cache()\n",
    "               \n",
    "            final_images = torch.cat([final_images, noise_images], dim=0)\n",
    "            final_labels = torch.cat([final_labels, labels], dim=0)\n",
    "            del noise_images\n",
    "            if GPU_AVAILABLE:\n",
    "                torch.cuda.empty_cache()\n",
    "      \n",
    "        print(f\"\\nOriginal dataset size: {n_original}\")\n",
    "        print(f\"Augmented dataset size: {len(final_images)}\")\n",
    "        print(f\"Augmentation multiplier: {len(final_images) / n_original:.1f}x\")\n",
    "        print(f\"Data stored on: {final_images.device}\")\n",
    "      \n",
    "        return final_images, final_labels\n",
    "  \n",
    "    def remove_outliers_iqr(self, images, labels):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"OUTLIER REMOVAL (IQR Method - GPU)\")\n",
    "        print(\"=\" * 70)\n",
    "      \n",
    "        batch_size = 100\n",
    "        mean_intensities_list = []\n",
    "       \n",
    "        for i in range(0, len(images), batch_size):\n",
    "            batch = images[i:i+batch_size]\n",
    "            batch_means = torch.mean(batch.view(len(batch), -1), dim=1)\n",
    "            mean_intensities_list.append(batch_means)\n",
    "            if GPU_AVAILABLE:\n",
    "                torch.cuda.empty_cache()\n",
    "       \n",
    "        mean_intensities = torch.cat(mean_intensities_list)\n",
    "        del mean_intensities_list\n",
    "        if GPU_AVAILABLE:\n",
    "            torch.cuda.empty_cache()\n",
    "      \n",
    "        Q1 = torch.quantile(mean_intensities, 0.25)\n",
    "        Q3 = torch.quantile(mean_intensities, 0.75)\n",
    "        IQR = Q3 - Q1\n",
    "      \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "      \n",
    "        mask = (mean_intensities >= lower_bound) & (mean_intensities <= upper_bound)\n",
    "       \n",
    "        indices = torch.where(mask)[0]\n",
    "       \n",
    "        print(f\"Original samples: {len(images)}\")\n",
    "        print(f\"Outliers removed: {(~mask).sum().item()}\")\n",
    "        print(f\"Remaining samples: {len(indices)}\")\n",
    "       \n",
    "        clean_images = torch.index_select(images, 0, indices)\n",
    "        clean_labels = torch.index_select(labels, 0, indices)\n",
    "       \n",
    "        del mask, indices, mean_intensities\n",
    "        if GPU_AVAILABLE:\n",
    "            torch.cuda.empty_cache()\n",
    "      \n",
    "        return clean_images, clean_labels\n",
    "  \n",
    "    def normalize_images(self, images):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"IMAGE NORMALIZATION (Z-Score - GPU)\")\n",
    "        print(\"=\" * 70)\n",
    "      \n",
    "        self.mean = torch.mean(images)\n",
    "        self.std = torch.std(images)\n",
    "      \n",
    "        print(f\"Global Mean: {self.mean.item():.2f}\")\n",
    "        print(f\"Global Std: {self.std.item():.2f}\")\n",
    "      \n",
    "        normalized = (images - self.mean) / (self.std + 1e-8)\n",
    "      \n",
    "        return normalized\n",
    "\n",
    "# ============================================================================\n",
    "# 3. RADIOMIC FEATURE EXTRACTION (GPU-Optimized)\n",
    "# ============================================================================\n",
    "class RadiomicFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "  \n",
    "    def extract_statistical_features_gpu(self, image):\n",
    "        features = []\n",
    "        features.append(torch.mean(image).item())\n",
    "        features.append(torch.var(image).item())\n",
    "        features.append(torch.std(image).item())\n",
    "        return torch.tensor(features, device=device)\n",
    "  \n",
    "    def compute_glcm_gpu(self, image, distance=1, angle=0):\n",
    "        levels = 16\n",
    "        image_quantized = (image / (256 / levels)).long()\n",
    "        image_quantized = torch.clamp(image_quantized, 0, levels - 1)\n",
    "        img_cpu = image_quantized.cpu().numpy()\n",
    "        glcm = np.zeros((levels, levels), dtype=np.float64)\n",
    "      \n",
    "        rows, cols = img_cpu.shape\n",
    "        if angle == 0:\n",
    "            dx, dy = 0, distance\n",
    "        elif angle == 45:\n",
    "            dx, dy = distance, distance\n",
    "        elif angle == 90:\n",
    "            dx, dy = distance, 0\n",
    "        else:\n",
    "            dx, dy = distance, -distance\n",
    "      \n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                ni, nj = i + dx, j + dy\n",
    "                if 0 <= ni < rows and 0 <= nj < cols:\n",
    "                    glcm[img_cpu[i, j], img_cpu[ni, nj]] += 1\n",
    "      \n",
    "        glcm = glcm / (np.sum(glcm) + 1e-10)\n",
    "        return torch.from_numpy(glcm).to(device)\n",
    "  \n",
    "    def extract_texture_features_gpu(self, image):\n",
    "        glcm = self.compute_glcm_gpu(image, distance=1, angle=0)\n",
    "        features = []\n",
    "        i_idx = torch.arange(glcm.shape[0], device=device).view(-1, 1).float()\n",
    "        j_idx = torch.arange(glcm.shape[1], device=device).view(1, -1).float()\n",
    "        contrast = torch.sum(glcm * (i_idx - j_idx) ** 2).item()\n",
    "        features.append(contrast)\n",
    "        homogeneity = torch.sum(glcm / (1 + (i_idx - j_idx) ** 2)).item()\n",
    "        features.append(homogeneity)\n",
    "        return torch.tensor(features, device=device)\n",
    "  \n",
    "    def gabor_kernel_gpu(self, ksize, sigma, theta, lambd, gamma, psi):\n",
    "        sigma_x = sigma\n",
    "        sigma_y = sigma / gamma\n",
    "        xmax = ksize // 2\n",
    "        ymax = ksize // 2\n",
    "        y = torch.arange(-ymax, ymax + 1, device=device).view(-1, 1).float()\n",
    "        x = torch.arange(-xmax, xmax + 1, device=device).view(1, -1).float()\n",
    "        x_theta = x * torch.cos(torch.tensor(theta, device=device)) + y * torch.sin(torch.tensor(theta, device=device))\n",
    "        y_theta = -x * torch.sin(torch.tensor(theta, device=device)) + y * torch.cos(torch.tensor(theta, device=device))\n",
    "        exp_part = torch.exp(-0.5 * (x_theta**2 / sigma_x**2 + y_theta**2 / sigma_y**2))\n",
    "        cos_part = torch.cos(2 * np.pi * x_theta / lambd + psi)\n",
    "        kernel = exp_part * cos_part\n",
    "        return kernel\n",
    "  \n",
    "    def extract_filter_features_gpu(self, image):\n",
    "        features = []\n",
    "        ksize = 21\n",
    "        sigma = 3\n",
    "        lambd = 10\n",
    "        gamma = 0.5\n",
    "        psi = 0\n",
    "        theta = 0\n",
    "        kernel = self.gabor_kernel_gpu(ksize, sigma, theta, lambd, gamma, psi)\n",
    "        img_4d = image.unsqueeze(0).unsqueeze(0)\n",
    "        kernel_4d = kernel.unsqueeze(0).unsqueeze(0)\n",
    "        filtered = torch.nn.functional.conv2d(img_4d, kernel_4d, padding=ksize//2)\n",
    "        filtered = filtered.squeeze()\n",
    "        mean_response = torch.mean(torch.abs(filtered)).item()\n",
    "        features.append(mean_response)\n",
    "        std_response = torch.std(filtered).item()\n",
    "        features.append(std_response)\n",
    "        return torch.tensor(features, device=device)\n",
    "  \n",
    "    def extract_all_features(self, images):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"RADIOMIC FEATURE EXTRACTION (GPU-Accelerated)\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"Feature Categories:\")\n",
    "        print(\" 1. Statistical: Mean, Variance, Std\")\n",
    "        print(\" 2. Texture: GLCM Contrast, Homogeneity\")\n",
    "        print(\" 3. Filter-based: Gabor Mean Response, Gabor Std Response\")\n",
    "        print(\"=\" * 70)\n",
    "      \n",
    "        all_features = []\n",
    "        n_images = len(images)\n",
    "        batch_size = 50\n",
    "      \n",
    "        for start_idx in range(0, n_images, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_images)\n",
    "            if start_idx % 500 == 0:\n",
    "                print(f\"Extracting features from image {start_idx}/{n_images}...\")\n",
    "           \n",
    "            batch_features = []\n",
    "            for idx in range(start_idx, end_idx):\n",
    "                img = images[idx]\n",
    "                img_min = torch.min(img)\n",
    "                img_max = torch.max(img)\n",
    "                img_uint8 = ((img - img_min) / (img_max - img_min + 1e-8) * 255)\n",
    "              \n",
    "                stat_features = self.extract_statistical_features_gpu(img_uint8)\n",
    "                texture_features = self.extract_texture_features_gpu(img_uint8)\n",
    "                filter_features = self.extract_filter_features_gpu(img_uint8)\n",
    "              \n",
    "                combined = torch.cat([stat_features, texture_features, filter_features])\n",
    "                batch_features.append(combined)\n",
    "           \n",
    "            all_features.append(torch.stack(batch_features))\n",
    "            if GPU_AVAILABLE:\n",
    "                torch.cuda.empty_cache()\n",
    "      \n",
    "        all_features = torch.cat(all_features, dim=0)\n",
    "      \n",
    "        print(f\"\\nTotal features extracted per image: {all_features.shape[1]}\")\n",
    "        print(f\"All features stored on: {all_features.device}\")\n",
    "      \n",
    "        return all_features\n",
    "\n",
    "# ============================================================================\n",
    "# 4. FEATURE FUSION AND SCALING (GPU)\n",
    "# ============================================================================\n",
    "class FeatureFusion:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "  \n",
    "    def scale_features(self, features, fit=True):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"FEATURE SCALING (GPU - Z-Score)\")\n",
    "        print(\"=\" * 70)\n",
    "      \n",
    "        if fit:\n",
    "            self.mean = torch.mean(features, dim=0)\n",
    "            self.std = torch.std(features, dim=0)\n",
    "            print(\"Fitted scaling parameters on GPU\")\n",
    "      \n",
    "        self.std[self.std == 0] = 1\n",
    "        scaled = (features - self.mean) / self.std\n",
    "      \n",
    "        print(f\"Scaled features shape: {scaled.shape}\")\n",
    "        print(f\"Mean: {torch.mean(scaled).item():.4f}\")\n",
    "        print(f\"Std: {torch.std(scaled).item():.4f}\")\n",
    "      \n",
    "        return scaled\n",
    "\n",
    "# ============================================================================\n",
    "# 5. DIMENSIONALITY REDUCTION (PCA on GPU)\n",
    "# ============================================================================\n",
    "class PCA:\n",
    "    def __init__(self, n_components=50):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "        self.explained_variance = None\n",
    "  \n",
    "    def fit(self, X):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"DIMENSIONALITY REDUCTION (PCA on GPU - {self.n_components} components)\")\n",
    "        print(\"=\" * 70)\n",
    "      \n",
    "        self.mean = torch.mean(X, dim=0)\n",
    "        X_centered = X - self.mean\n",
    "        cov_matrix = torch.mm(X_centered.T, X_centered) / (X.shape[0] - 1)\n",
    "        eigenvalues, eigenvectors = torch.linalg.eigh(cov_matrix)\n",
    "        idx = torch.argsort(eigenvalues, descending=True)\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "        self.components = eigenvectors[:, :self.n_components]\n",
    "        self.explained_variance = eigenvalues[:self.n_components]\n",
    "        total_var = torch.sum(eigenvalues)\n",
    "        explained_var_ratio = self.explained_variance / total_var\n",
    "        cumulative_var = torch.cumsum(explained_var_ratio, dim=0)\n",
    "      \n",
    "        print(f\"Original dimension: {X.shape[1]}\")\n",
    "        print(f\"Reduced dimension: {self.n_components}\")\n",
    "        print(f\"Total variance explained: {cumulative_var[-1].item():.4f}\")\n",
    "        print(f\"Computation done on: GPU\")\n",
    "      \n",
    "        return self\n",
    "  \n",
    "    def transform(self, X):\n",
    "        X_centered = X - self.mean\n",
    "        return torch.mm(X_centered, self.components)\n",
    "  \n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "# ============================================================================\n",
    "# 6. DATA SPLITTING (GPU)\n",
    "# ============================================================================\n",
    "class DataSplitter:\n",
    "    @staticmethod\n",
    "    def shuffle_data(X, y, seed=42):\n",
    "        torch.manual_seed(seed)\n",
    "        indices = torch.randperm(len(X), device=device)\n",
    "        return X[indices], y[indices]\n",
    "  \n",
    "    @staticmethod\n",
    "    def train_test_split(X, y, train_ratio=0.7, seed=42):\n",
    "        X_shuffled, y_shuffled = DataSplitter.shuffle_data(X, y, seed)\n",
    "        n_samples = len(X)\n",
    "        n_train = int(n_samples * train_ratio)\n",
    "        return X_shuffled[:n_train], X_shuffled[n_train:], y_shuffled[:n_train], y_shuffled[n_train:]\n",
    "\n",
    "# ============================================================================\n",
    "# 7. LOGISTIC REGRESSION CLASSIFIER (Full GPU Implementation) - FIXED\n",
    "# ============================================================================\n",
    "class LogisticRegressionGPU:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=2000, regularization=0.01):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iterations\n",
    "        self.l2 = regularization\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.n_classes = None\n",
    "        self.training_history = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"TRAINING: Logistic Regression on GPU (Multiclass)\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.n_classes = len(torch.unique(y))\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.weights = torch.randn(n_features, self.n_classes, device=device) * 0.01\n",
    "        self.bias = torch.zeros(self.n_classes, device=device)\n",
    "\n",
    "        # One-hot encode labels\n",
    "        y_onehot = torch.zeros(n_samples, self.n_classes, device=device)\n",
    "        y_onehot[torch.arange(n_samples), y] = 1\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            linear = torch.mm(X, self.weights) + self.bias\n",
    "            y_pred = torch.softmax(linear, dim=1)\n",
    "\n",
    "            # Cross-entropy loss + L2\n",
    "            loss = -torch.mean(torch.sum(y_onehot * torch.log(y_pred + 1e-8), dim=1))\n",
    "            loss += self.l2 * torch.sum(self.weights ** 2)\n",
    "\n",
    "            # Gradients\n",
    "            grad_w = (1/n_samples) * torch.mm(X.t(), (y_pred - y_onehot)) + 2 * self.l2 * self.weights\n",
    "            grad_b = (1/n_samples) * torch.sum(y_pred - y_onehot, dim=0)\n",
    "\n",
    "            # Update\n",
    "            self.weights -= self.lr * grad_w\n",
    "            self.bias -= self.lr * grad_b\n",
    "\n",
    "            if i % 500 == 0:\n",
    "                acc = (torch.argmax(y_pred, dim=1) == y).float().mean().item()\n",
    "                print(f\"  Iteration {i}: Loss = {loss.item():.4f}, Accuracy = {acc:.4f}\")\n",
    "\n",
    "        # Store class distribution\n",
    "        for c in range(self.n_classes):\n",
    "            count = (y == c).sum().item()\n",
    "            self.training_history.append({'class': int(c), 'n_samples': count, 'prior': count / n_samples})\n",
    "\n",
    "        print(\"Logistic Regression training completed on GPU!\")\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        linear = torch.mm(X, self.weights) + self.bias\n",
    "        return torch.softmax(linear, dim=1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return torch.argmax(self.predict_proba(X), dim=1)\n",
    "\n",
    "# ============================================================================\n",
    "# 8. MODEL EVALUATION\n",
    "# ============================================================================\n",
    "class ModelEvaluator:\n",
    "    @staticmethod\n",
    "    def confusion_matrix(y_true, y_pred, n_classes=3):\n",
    "        if isinstance(y_true, torch.Tensor):\n",
    "            y_true = y_true.cpu().numpy()\n",
    "        if isinstance(y_pred, torch.Tensor):\n",
    "            y_pred = y_pred.cpu().numpy()\n",
    "        cm = np.zeros((n_classes, n_classes), dtype=int)\n",
    "        for true, pred in zip(y_true, y_pred):\n",
    "            cm[int(true), int(pred)] += 1\n",
    "        return cm\n",
    "  \n",
    "    @staticmethod\n",
    "    def accuracy(y_true, y_pred):\n",
    "        if isinstance(y_true, torch.Tensor) and isinstance(y_pred, torch.Tensor):\n",
    "            return (y_true == y_pred).float().mean().item()\n",
    "        return float(np.mean(y_true == y_pred))\n",
    "  \n",
    "    @staticmethod\n",
    "    def precision_recall_f1(y_true, y_pred, n_classes=3):\n",
    "        cm = ModelEvaluator.confusion_matrix(y_true, y_pred, n_classes)\n",
    "        precision = np.zeros(n_classes)\n",
    "        recall = np.zeros(n_classes)\n",
    "        f1 = np.zeros(n_classes)\n",
    "        for i in range(n_classes):\n",
    "            tp = cm[i, i]\n",
    "            fp = np.sum(cm[:, i]) - tp\n",
    "            fn = np.sum(cm[i, :]) - tp\n",
    "            precision[i] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall[i] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i]) if (precision[i] + recall[i]) > 0 else 0\n",
    "        return precision, recall, f1\n",
    "  \n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(cm, class_names, filename):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "  \n",
    "    @staticmethod\n",
    "    def plot_roc_curve(y_true, y_proba, n_classes, class_names, filename):\n",
    "        if isinstance(y_true, torch.Tensor):\n",
    "            y_true = y_true.cpu().numpy()\n",
    "        if isinstance(y_proba, torch.Tensor):\n",
    "            y_proba = y_proba.cpu().numpy()\n",
    "      \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        for i in range(n_classes):\n",
    "            y_true_binary = (y_true == i).astype(int)\n",
    "            y_scores = y_proba[:, i]\n",
    "            thresholds = np.linspace(0, 1, 100)\n",
    "            tpr_list = []\n",
    "            fpr_list = []\n",
    "            for thresh in thresholds:\n",
    "                y_pred_binary = (y_scores >= thresh).astype(int)\n",
    "                tp = np.sum((y_true_binary == 1) & (y_pred_binary == 1))\n",
    "                fp = np.sum((y_true_binary == 0) & (y_pred_binary == 1))\n",
    "                tn = np.sum((y_true_binary == 0) & (y_pred_binary == 0))\n",
    "                fn = np.sum((y_true_binary == 1) & (y_pred_binary == 0))\n",
    "                tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "                tpr_list.append(tpr)\n",
    "                fpr_list.append(fpr)\n",
    "           \n",
    "            fpr_array = np.array(fpr_list)\n",
    "            tpr_array = np.array(tpr_list)\n",
    "            sorted_idx = np.argsort(fpr_array)\n",
    "            fpr_sorted = fpr_array[sorted_idx]\n",
    "            tpr_sorted = tpr_array[sorted_idx]\n",
    "            auc = np.trapz(tpr_sorted, fpr_sorted)\n",
    "            plt.plot(fpr_sorted, tpr_sorted, label=f'{class_names[i]} (AUC = {auc:.3f})', linewidth=2)\n",
    "      \n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves - Multiclass Classification')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "  \n",
    "    @staticmethod\n",
    "    def plot_class_distribution(training_history, filename):\n",
    "        classes = [h['class'] for h in training_history]\n",
    "        n_samples = [h['n_samples'] for h in training_history]\n",
    "        priors = [h['prior'] for h in training_history]\n",
    "      \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        ax1.bar(classes, n_samples, color=['blue', 'orange', 'green'])\n",
    "        ax1.set_xlabel('Class')\n",
    "        ax1.set_ylabel('Number of Samples')\n",
    "        ax1.set_title('Training Samples per Class')\n",
    "        ax1.set_xticks(classes)\n",
    "        ax1.grid(alpha=0.3)\n",
    "      \n",
    "        ax2.bar(classes, priors, color=['blue', 'orange', 'green'])\n",
    "        ax2.set_xlabel('Class')\n",
    "        ax2.set_ylabel('Prior Probability')\n",
    "        ax2.set_title('Class Prior Probabilities')\n",
    "        ax2.set_xticks(classes)\n",
    "        ax2.grid(alpha=0.3)\n",
    "      \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 9. MAIN PIPELINE (Full CUDA with PyTorch)\n",
    "# ============================================================================\n",
    "def main():\n",
    "    output_dir = \"output_lr_cuda\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "  \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CUDA-ACCELERATED LUNG DISEASE CLASSIFICATION\")\n",
    "    print(\"PyTorch CUDA - All Processing on GPU - LOGISTIC REGRESSION\")\n",
    "    print(\"=\" * 70)\n",
    "  \n",
    "    if GPU_AVAILABLE:\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"\\nInitial GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
    "        print(f\"Initial GPU Memory: {torch.cuda.memory_reserved() / 1e9:.2f} GB reserved\")\n",
    "  \n",
    "    dataset_path = r'C:\\Users\\Wolf\\PAGANI\\chest_xray'\n",
    "    loader = DatasetLoader(dataset_path)\n",
    "    images, labels = loader.load_dataset(target_size=(256, 256), max_samples_per_class=None)\n",
    "  \n",
    "    if GPU_AVAILABLE:\n",
    "        print(f\"GPU Memory after loading: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
    "  \n",
    "    preprocessor = DataPreprocessor()\n",
    "\n",
    "    # CHANGED: augmentation_factor=2\n",
    "    images_aug, labels_aug = preprocessor.augment_data(images, labels, augmentation_factor=2)\n",
    "  \n",
    "    del images, labels\n",
    "    if GPU_AVAILABLE:\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"GPU Memory after augmentation: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
    "  \n",
    "    images_clean, labels_clean = preprocessor.remove_outliers_iqr(images_aug, labels_aug)\n",
    "    del images_aug, labels_aug\n",
    "    if GPU_AVAILABLE:\n",
    "        torch.cuda.empty_cache()\n",
    "   \n",
    "    images_norm = preprocessor.normalize_images(images_clean)\n",
    "    del images_clean\n",
    "    if GPU_AVAILABLE:\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"GPU Memory after preprocessing: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
    "  \n",
    "    feature_extractor = RadiomicFeatureExtractor()\n",
    "    features = feature_extractor.extract_all_features(images_norm)\n",
    "    del images_norm\n",
    "    if GPU_AVAILABLE:\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"GPU Memory after feature extraction: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
    "  \n",
    "    fusion = FeatureFusion()\n",
    "    features_scaled = fusion.scale_features(features, fit=True)\n",
    "    del features\n",
    "    if GPU_AVAILABLE:\n",
    "        torch.cuda.empty_cache()\n",
    "  \n",
    "    pca = PCA(n_components=min(7, features_scaled.shape[1]))\n",
    "    features_pca = pca.fit_transform(features_scaled)\n",
    "    del features_scaled\n",
    "    if GPU_AVAILABLE:\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"GPU Memory after PCA: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
    "  \n",
    "    class_names = ['Normal', 'Bacterial Pneumonia', 'Viral Pneumonia']\n",
    "    train_ratio = 0.7\n",
    "  \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"TRAINING WITH {int(train_ratio*100)}% TRAIN / {int((1-train_ratio)*100)}% TEST SPLIT\")\n",
    "    print(\"=\" * 70)\n",
    "  \n",
    "    X_train, X_test, y_train, y_test = DataSplitter.train_test_split(\n",
    "        features_pca, labels_clean, train_ratio=train_ratio\n",
    "    )\n",
    "  \n",
    "    print(f\"\\nTrain set: {len(X_train)} samples (GPU)\")\n",
    "    print(f\"Test set: {len(X_test)} samples (GPU)\")\n",
    "  \n",
    "    if GPU_AVAILABLE:\n",
    "        print(f\"GPU Memory before training: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
    "  \n",
    "    start_time = time.time()\n",
    "  \n",
    "    # Now using proper Logistic Regression\n",
    "    lr_model = LogisticRegressionGPU(learning_rate=0.01, n_iterations=2000, regularization=0.01)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "  \n",
    "    train_time = time.time() - start_time\n",
    "  \n",
    "    if GPU_AVAILABLE:\n",
    "        print(f\"\\nGPU Memory after training: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
    "  \n",
    "    y_pred = lr_model.predict(X_test)\n",
    "    y_proba = lr_model.predict_proba(X_test)\n",
    "  \n",
    "    cm = ModelEvaluator.confusion_matrix(y_test, y_pred)\n",
    "    accuracy = ModelEvaluator.accuracy(y_test, y_pred)\n",
    "    precision, recall, f1 = ModelEvaluator.precision_recall_f1(y_test, y_pred)\n",
    "  \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"RESULTS (CUDA-Accelerated with PyTorch)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Training Time: {train_time:.2f}s\")\n",
    "    if GPU_AVAILABLE:\n",
    "        print(f\"Processing: 100% on {torch.cuda.get_device_name(0)}\")\n",
    "  \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"\\n{class_name}:\")\n",
    "        print(f\"  Precision: {precision[i]:.4f}\")\n",
    "        print(f\"  Recall: {recall[i]:.4f}\")\n",
    "        print(f\"  F1-Score: {f1[i]:.4f}\")\n",
    "  \n",
    "    cm_filename = os.path.join(output_dir, f'confusion_matrix_LR_CUDA_{int(train_ratio*100)}.png')\n",
    "    ModelEvaluator.plot_confusion_matrix(cm, class_names, cm_filename)\n",
    "  \n",
    "    roc_filename = os.path.join(output_dir, f'roc_curve_LR_CUDA_{int(train_ratio*100)}.png')\n",
    "    ModelEvaluator.plot_roc_curve(y_test, y_proba, 3, class_names, roc_filename)\n",
    "  \n",
    "    class_dist_filename = os.path.join(output_dir, f'class_distribution_LR_CUDA_{int(train_ratio*100)}.png')\n",
    "    ModelEvaluator.plot_class_distribution(lr_model.training_history, class_dist_filename)\n",
    "  \n",
    "    results = {\n",
    "        'model': 'Logistic Regression (PyTorch CUDA-Accelerated)',\n",
    "        'gpu': torch.cuda.get_device_name(0) if GPU_AVAILABLE else 'CPU',\n",
    "        'train_ratio': train_ratio,\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision': precision.tolist(),\n",
    "        'recall': recall.tolist(),\n",
    "        'f1_score': f1.tolist(),\n",
    "        'training_time': float(train_time),\n",
    "        'confusion_matrix': cm.tolist()\n",
    "    }\n",
    "  \n",
    "    results_file = os.path.join(output_dir, 'results_summary_lr_cuda.json')\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "  \n",
    "    print(f\"\\nResults saved to: {results_file}\")\n",
    "    print(f\"Visualizations saved to: {output_dir}/\")\n",
    "  \n",
    "    if GPU_AVAILABLE:\n",
    "        print(f\"\\nFinal GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
    "        print(f\"Peak GPU Memory: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "  \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CUDA-ACCELERATED PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef8ae1-955c-481a-9f90-3565e8f87b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
